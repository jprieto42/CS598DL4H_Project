{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586d2e33",
   "metadata": {},
   "source": [
    "Github repo: https://github.com/jprieto42/CS598DL4H_Project\n",
    "\n",
    "Video link : https://youtu.be/jR8m8GHRGF0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efcf5b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In Proteomics one of the challenges in proteomic analysis is the stochastic nature of peptide selection in mass spectrometry (MS). Peptide detectability, which is the ability of a peptide to be accurately detected by MS, is necessary to identify the protein and quantify it. Existing methods often overlook peptide detectability, leading to inaccurate results and hindered progress in proteomic research. The paper presents DeepMSPeptide, a bioinformatic tool that uses a deep learning method to predict which peptides can be detected in MS exclusively based on the peptide amino acid sequences.\n",
    "\n",
    "The specific approach of DeepMSPeptide utilizes a convolutional neural network (CNN) architecture to predict the detectability of peptides based solely on their amino acid sequences. The input vector of the DeepMSPeptide classifier is created using a CNN in TensorFlow by assigning an integer to each amino acid. The input is standardized for the CNN by performing zero-padding because of the different lengths of the peptides which results in each vector with a size of 81 elements. The first layer of the model is an embedding layer. The second layer is a dropout layer to prevent over-fitting. The next layers include 1 or 2 1D convolutional layers applying 64 filters with a sliding window of 1 and a width of 3. Lastly, the model includes two additional hidden layers. Those layers include a single-unit layer with a sigmoid activation function and the loss function selected for training was the binary cross entropy. Each model is trained for 200 epochs in batches of 100 samples. The accuracy is measured during the training with a fourth of the samples as the validation set and a call-back function to stop the training when the loss function is stabilized during five consecutive epochs. The input data used to train and test the different classifier approaches were obtained from the GPMDB database. This papers contribution was specifically adding CNNs to better learn the relationships of the amino acid sequences before passing them into the deep learning model. Previously the best performing model was that of random forrests but this papers model was able to surpass them. The importance of this contribution is that they have furthered the performance in the ability to predict petide detectability using deep learning methods which will ultimately help researchers make more discoveries in cell biology and human disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b70c01",
   "metadata": {},
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "The paper does not explicitly list the hypothesis being tested but these are implied by its methods being tested.\n",
    "\n",
    "Hypothesis 1: The CNN-DL will outperform previous existing machine learning (ML) models.\n",
    "- To test this I will compare performance statistics of the models against performance statistics of previous ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdddbb",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a16c25",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "- Source of the data: The input data used to train and test the different classifier approaches were obtained from the GPMDB database, which includes thousands of peptides detected in hundreds of MS/MS experiments and the frequency of detection of each of them.[1] The data used to build the model is not needed for the testing of the model because it is pre trained and compiled.\n",
    "- Statistics: The statisctics for the train and test datasets are includede below.The split to train the model was 75% train 25 % test.\n",
    "- Data process: The data must be processed first by reading the first and last columns of the GPMDB dataset that correspond to the peptide sequence and class respectively. The class consist of two classes, MObs which is a peptide that is most observed by MS and LObs which is a peptide that is least observed by MS. The classes are then converted to binary to match adapted code output, MObs = 1 and LObs = 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c48d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91f9e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir and function to load raw data\n",
    "\n",
    "def load_pep_and_codify(file, max_len):\n",
    "    aa_dict={'A':1,'R':2,'N':3,'D':4,'C':5,'Q':6,'E':7,'G':8,'H':9,'I':10,'L':11,'K':12,'M':13,'F':14,\n",
    "        'P':15,'O':16,'S':17,'U':18,'T':19,'W':20,'Y':21,'V':22}\n",
    "    with open(file, 'r') as inf:\n",
    "        lines = inf.read().splitlines()\n",
    "    pep_codes=[]\n",
    "    long_pep_counter = 0\n",
    "    newLines = []\n",
    "    for pep in lines:\n",
    "        if not len(pep) > max_len:\n",
    "            current_pep=[]\n",
    "            for aa in pep:\n",
    "                current_pep.append(aa_dict[aa])\n",
    "            pep_codes.append(current_pep)\n",
    "            newLines.extend([pep])\n",
    "        else:\n",
    "            long_pep_counter += 1\n",
    "    predict_data = keras.preprocessing.sequence.pad_sequences(pep_codes, value=0, padding='post', maxlen=max_len)\n",
    "    return predict_data, long_pep_counter, newLines\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60addbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertClass(stri):\n",
    "    if stri == 'MObs':\n",
    "        return 1\n",
    "    elif stri == 'LObs':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def read_data_display_stat(ifile):\n",
    "    class_dict = {}\n",
    "    with open (ifile, 'r') as f:\n",
    "        read_data = [(column[0],convertClass(column[-1])) for column in csv.reader(f,delimiter='\\t')]\n",
    "    #print (first_row)\n",
    "    read_data.pop(0)\n",
    "\n",
    "    #calc statistics\n",
    "    MObs_total = 0\n",
    "    LObs_total = 0\n",
    "    pep_lengths = []\n",
    "    for element in read_data:\n",
    "        if element[1]==1:\n",
    "            MObs_total+=1\n",
    "        if element[1]==0:\n",
    "            LObs_total+=1\n",
    "        pep_lengths.append(len(element[0]))\n",
    "        class_dict[element[0]] = element[1]\n",
    "\n",
    "    vals, counts = np.unique(pep_lengths, return_counts=True)\n",
    "\n",
    "    #find mode\n",
    "    mode_value = np.argwhere(counts == np.max(counts))\n",
    "    #Display data statistics\n",
    "    print()\n",
    "    print(ifile)\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Total Size : {0} {1}\".format(len(read_data),\"peptides\"))\n",
    "    print(\"Average Peptide Length : {}\".format(np.mean(pep_lengths)))\n",
    "    print(\"Maximum Peptide Length : {}\".format(np.max(pep_lengths)))\n",
    "    print(\"Minimum Peptide Length : {}\".format(np.min(pep_lengths)))\n",
    "    print(\"Median Peptide Length : {}\".format(np.median(pep_lengths)))\n",
    "    print(\"Mode Peptide Length : {}\".format(mode_value[0][0]))\n",
    "    print(\"Class Breakdown\")\n",
    "    print(\"MObs : {0} %\".format((MObs_total/len(read_data))*100))\n",
    "    print(\"LObs : {0} %\".format((LObs_total/len(read_data))*100))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    return read_data,class_dict\n",
    "\n",
    "#Create model input file\n",
    "def create_model_infile(input_fname,read_list):    \n",
    "    with open(input_fname, 'w') as outfile:\n",
    "        outfile.writelines(str(element[0]) + '\\n' for element in read_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb948462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPMDB_training_peptides.txt\n",
      "--------------------------------------------------------\n",
      "Total Size : 74998 peptides\n",
      "Average Peptide Length : 15.946931918184484\n",
      "Maximum Peptide Length : 81\n",
      "Minimum Peptide Length : 4\n",
      "Median Peptide Length : 14.0\n",
      "Mode Peptide Length : 7\n",
      "Class Breakdown\n",
      "MObs : 49.99866663111016 %\n",
      "LObs : 50.00133336888983 %\n",
      "--------------------------------------------------------\n",
      "\n",
      "GPMDB_test_peptides.txt\n",
      "--------------------------------------------------------\n",
      "Total Size : 24997 peptides\n",
      "Average Peptide Length : 15.970356442773133\n",
      "Maximum Peptide Length : 79\n",
      "Minimum Peptide Length : 4\n",
      "Median Peptide Length : 14.0\n",
      "Mode Peptide Length : 7\n",
      "Class Breakdown\n",
      "MObs : 49.993999279913595 %\n",
      "LObs : 50.006000720086405 %\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "raw_data_dir_train = 'GPMDB_training_peptides.txt'\n",
    "raw_data_dir_test = 'GPMDB_test_peptides.txt'\n",
    "input_file_name = \"test_input.txt\"\n",
    "read_data_display_stat(raw_data_dir_train)\n",
    "datalist,truth_dict = read_data_display_stat(raw_data_dir_test)\n",
    "create_model_infile(input_file_name,datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b54693f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded 24997 peptides and skipped 0\n"
     ]
    }
   ],
   "source": [
    "predict_data, skipped,  lines = load_pep_and_codify(input_file_name, 81)\n",
    "#print(predict_data)\n",
    "print('Succesfully loaded {0} peptides and skipped {1}'.format(len(lines), str(skipped)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d36270",
   "metadata": {},
   "source": [
    "## Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "\n",
    "- Model architecture: The input vector of the DeepMSPeptide classifier is created using a CNN in TensorFlow by assigning an integer to each amino acid. The input is standardized for the CNN by performing zero-padding because of the different lengths of the peptides which results in each vector with a size of 81 elements. The first layer of the model is an embedding layer. The second layer is a dropout layer to prevent over-fitting. The next layers include 1 or 2 1D convolutional layers applying 64 filters with a sliding window of 1 and a width of 3. Lastly, the model includes two additional hidden layers. Those layers include a single-unit layer with a sigmoid activation function and the loss function selected for training was the binary cross entropy. Each model is trained for 200 epochs in batches of 100 samples. \n",
    "- Training objectives: The model is pre trained and compiled\n",
    "- Others: The model is pre trained and compiled\n",
    "Code is partially adapted from https://github.com/vsegurar/DeepMSPeptide/blob/master/DeepMSPeptide/DeepMSPeptide.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95478a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "Making predictions\n",
      "Saving predictions to file out_Predictions.txt\n"
     ]
    }
   ],
   "source": [
    "file_name = \"out.txt\"\n",
    "\n",
    "print('Loading model...')\n",
    "model_2_1D = keras.models.load_model('model_2_1D.h5')\n",
    "\n",
    "\n",
    "print('Making predictions')\n",
    "model_2_1D_pred = model_2_1D.predict(predict_data)\n",
    "model_2_1D_pred = np.hstack((np.array(lines).reshape(len(lines), 1),model_2_1D_pred)).tolist()\n",
    "\n",
    "Pred_output = []\n",
    "for pred in model_2_1D_pred:\n",
    "    if float(pred[1]) > 0.5:\n",
    "        # pred.extend('0')\n",
    "        Pred_output.append([pred[0], str(1-float(pred[1])), '0'])\n",
    "    else:\n",
    "        Pred_output.append([pred[0], str(1-float(pred[1])), '1'])\n",
    "        # pred.extend('1')\n",
    "\n",
    "outFile = '{0}_Predictions.txt'.format(file_name.split('.')[0])\n",
    "print('Saving predictions to file {}'.format(outFile))\n",
    "with open(outFile, 'w') as outf:\n",
    "    outf.write('Peptide\\tProb\\tDetectability\\n')\n",
    "    outf.writelines('\\t'.join(i) + '\\n' for i in Pred_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266a565",
   "metadata": {},
   "source": [
    "# Results\n",
    "Run cell below to show results. Measures include Confusion matrix values, Accuracy, Precision, Recall/Sensitivity, F1 Score, Specificity, and AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a1a78c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "True Positive : 11097\n",
      "True Negative : 8784\n",
      "False Positive : 3716\n",
      "False Negative : 1400\n",
      "--------------------------------------------\n",
      "Accuracy : 0.7953354402528303\n",
      "Precision : 0.7491392695605211\n",
      "Recall/Specificity : 0.8879731135472514\n",
      "F1 Score : 0.8126693518857561\n",
      "Sensitivity : 0.70272\n",
      "AUC : 0.7953465567736256\n"
     ]
    }
   ],
   "source": [
    "with open (outFile, 'r') as f:\n",
    "        predictions = [(column[0],column[-1]) for column in csv.reader(f,delimiter='\\t')]\n",
    "        \n",
    "predictions.pop(0)\n",
    "labels = []\n",
    "pred = [] \n",
    "ind = 0\n",
    "tp_count = 0\n",
    "tn_count = 0\n",
    "fp_count = 0\n",
    "fn_count = 0\n",
    "for element in predictions:\n",
    "    labels.append(truth_dict[element[0]]) \n",
    "    pred.append(int(element[1]))\n",
    "    if truth_dict[element[0]] == int(element[1]):\n",
    "        if int(element[1])==1:\n",
    "            tp_count+=1\n",
    "        else:\n",
    "            tn_count+=1\n",
    "    else:\n",
    "        if int(element[1])==1:\n",
    "            fp_count+=1\n",
    "        else:\n",
    "            fn_count+=1\n",
    "      \n",
    "#print(labels)\n",
    "#print(pred)\n",
    "print(\"--------------------------------------------\")    \n",
    "print(\"True Positive : {}\".format(tp_count))\n",
    "print(\"True Negative : {}\".format(tn_count))\n",
    "print(\"False Positive : {}\".format(fp_count))\n",
    "print(\"False Negative : {}\".format(fn_count))\n",
    "\n",
    "labels = np.array(labels)\n",
    "pred = np.array(pred)\n",
    "\n",
    "accuracy = (tp_count+tn_count)/(tp_count+tn_count+fp_count+fn_count)\n",
    "precision = tp_count/(tp_count+fp_count)\n",
    "recall = tp_count/(tp_count+fn_count)\n",
    "f1_score = (2*precision*recall)/(precision+recall)\n",
    "sensitivity = tn_count / (tn_count + fp_count)\n",
    "auc = metrics.roc_auc_score(labels, pred)\n",
    "print(\"--------------------------------------------\")  \n",
    "print(\"Accuracy : {}\".format(accuracy))\n",
    "print(\"Precision : {}\".format(precision))\n",
    "print(\"Recall/Specificity : {}\".format(recall))\n",
    "print(\"F1 Score : {}\".format(f1_score))\n",
    "print(\"Sensitivity : {}\".format(sensitivity))\n",
    "print(\"AUC : {}\".format(auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287de7d6",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa87fa",
   "metadata": {},
   "source": [
    "This table was retrieved from the paper [1]\n",
    "\n",
    "|Model|\tAUC|\tAccuracy|\tSpecificity|\tSensitivity|\tF-score|\n",
    "|---|---|---|---|---|---|\n",
    "|This Project|0.7953|0.7953 |0.8880 | 0.7027|0.8126|\n",
    "|1D-2C-CNN\t|0.8570\t|0.7953\t|0.8880\t|0.7027\t|0.7744|\n",
    "|1D-1C-CNN\t|0.8568\t|0.7917\t|0.9097\t|0.6737\t|0.7638|\n",
    "|RFa\t|0.7549\t|0.6924\t|0.7746\t|0.6103\t|0.6649|\n",
    "|SvmRa\t|0.7384\t|0.6813\t|0.7830\t|0.5797\t|0.6453|\n",
    "|DNNb\t|0.7360\t|0.6692\t|0.6813\t|0.6572\t|0.6659|\n",
    "|C5a\t|0.7312\t|0.6644\t|0.6513\t|0.6775\t|0.6687|\n",
    "|Nneta\t|0.7148\t|0.6723\t|0.8329\t|0.5118\t|0.6097|\n",
    "|Rparta\t|0.6893\t|0.6527\t|0.7467\t|0.5587\t|0.6167|\n",
    "|Nba\t|0.6456\t|0.5997\t|0.7280\t|0.4714\t|0.5408|\n",
    "|Plsa\t|0.6350\t|0.6043\t|0.6396\t|0.5690\t|0.5898|\n",
    "|Glma\t|0.6349\t|0.6036\t|0.6426\t|0.5646\t|0.5875|\n",
    "|GlmStepAICa\t|0.6349\t|0.6034\t|0.6424\t|0.5645\t|0.5874|\n",
    "|Gaussianc\t|0.6342\t|0.5983\t|0.6121\t|0.5845\t|0.5927|\n",
    "|Jripa\t|0.6238\t|0.6240\t|0.6549\t|0.5930\t|0.6119|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38fa87",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "   This project was able to partially reproduce the results of this paper. I was able to achieve the same accuracy, specificity, and sensitivity, however I got different results for AUC and F-score. The results I was able to achieve was still better than the other models that were being compared. Future work in this area could be testing the effects of extra layers on the effectiveness of the model to predict observabilty of the peptides. The easy part about reproducing this paper is that the model provided in the github was pre trained and pre compiled so that simplified things. The difficult part about reproducing the paper was that the paper didn't provide instructions on how to test the model. It told us how to use a python script that was written to take and input file and produce an output but did not specify details of the input format. I had to figure out on my own how to clean the test data to create the input file. The suggestions I will give to the author are to be more descriptive in the github read.ME. It would be nice to have clear instructions and even code on how to reproduce the results of the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf984726",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Serrano, G., Guruceaga, E., & Segura, V. (2020). DeepMSPeptide: peptide detectability prediction using deep learning. Bioinformatics, 36(4), 1279–1280. doi: 10.1093/bioinformatics/btz708. Advance Access Publication Date: September 14, 2019. Available online: https://academic.oup.com/bioinformatics/article/36/4/1279/5569881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def28c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs598DL",
   "language": "python",
   "name": "cs598dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
